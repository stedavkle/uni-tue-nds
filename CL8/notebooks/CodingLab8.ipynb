{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Neural Data Science_\n",
    "\n",
    "Lecturer: Jan Lause, Prof. Dr. Philipp Berens\n",
    "\n",
    "Tutors: Jonas Beck, Rita González Márquez, Fabio Seel\n",
    "\n",
    "Summer term 2024\n",
    "\n",
    "Name: FILL IN YOUR NAMES HERE\n",
    "\n",
    "# Coding Lab 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2024-04-12 15:29:57CEST\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.12.2\n",
      "IPython version      : 8.22.2\n",
      "\n",
      "sklearn: 1.4.1.post1\n",
      "\n",
      "numpy     : 1.26.4\n",
      "matplotlib: 3.8.3\n",
      "seaborn   : 0.13.2\n",
      "\n",
      "Watermark: 2.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "mpl.rc(\"savefig\", dpi=72)\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext jupyter_black\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark --time --date --timezone --updated --python --iversions --watermark -p sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Implement entropy estimators\n",
    "\n",
    "\n",
    "*Grading: 5 pts (basic) + 1 pts (advanced) + 3 pts (bonus)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General framework\n",
    "\n",
    "Entropy is defined as \n",
    "\n",
    "$$\n",
    "H[p] = -\\sum_x p_x \\log p_x\n",
    "$$\n",
    "\n",
    "where $p_x = p(x=X)$. Here we assume that $X$ is a discrete random variable and that there are finitely many states $K$ that $X$ can take.\n",
    "\n",
    "We are interested in the entropy of discrete random variables, because of its relationship with mutual information:\n",
    "\n",
    "$$\n",
    "I[X|Y] = H[X] - H[X|Y]\n",
    "$$\n",
    "\n",
    "If we can estimate the entropy well, we can estimate the mutual information well. An application in neuroscience would be estimating the mutual information between a spike train modeled as a sequence of $1$ s and $0$ s (e.g. $(0,1,0,1,1)$) and a discrete set of stimuli.\n",
    "\n",
    "Note that a multivariate binary distribution modeling a spike train can always be mapped to a discrete univariate distribution, $\\mathbb{Z}_2 \\longrightarrow \\mathbb{Z_+}$, by interpreting each binary state $z \\in \\mathbb{Z}_2$ as its corresponding binary number and computing $f(z) = \\sum_i 2^{i} z_i$.\n",
    "\n",
    "As discussed in the lecture, the problem is that one always underestimates the true entropy of a distribution from samples. In this exercise you are meant to implement different estimators for discrete entropy and evaluate them on different discrete distributions:\n",
    "\n",
    "* Uniform distribution: $p(x=X) = \\frac{1}{K}$\n",
    "\n",
    "* \"Zipf's law\"- distribution: $p(x=X) = \\frac{1}{Z x} $, where $Z = \\sum_k 1/k$\n",
    "\n",
    "There is a really good series of blog posts about discrete entropy estimation to be found [here](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-1.html), [here](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-2.html) and [here](http://www.nowozin.net/sebastian/blog/estimating-discrete-entropy-part-3.html). \n",
    "\n",
    "Make sure you use binary logarithms throughout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the estimators\n",
    "\n",
    "Implement the\n",
    "\n",
    "* maximum likelihood estimator (1 pt)\n",
    "* miller-maddow corrected estimator (1 pt)\n",
    "* jack-knife corrected estimator (2 pt)\n",
    "* coverage adjusted estimator (1 pt).\n",
    "\n",
    "When implementing the jack-knife estimator, you may want to restrict the amount of resampling for performance reasons e.g. to 1000, even if more samples are available. By definition, $0\\log0=0$. Adapt the interfaces as needed for your implementation.\n",
    "\n",
    "In addition, implement or use one of the following more advanced estimators (1+3 pts, extra points if you use your own implementation):\n",
    "\n",
    "* [JVHW estimator](https://arxiv.org/abs/1406.6956) with code on [github](https://github.com/EEthinker/JVHW_Entropy_Estimators/tree/master/Python)\n",
    "* [Unseen estimator](http://papers.nips.cc/paper/5170-estimating-the-unseen-improved-estimators-for-entropy-and-other-properties) (includes Matlab code in Supplementary)\n",
    "* [Best Upper Bounds estimator](http://www.mitpressjournals.org/doi/abs/10.1162/089976603321780272) (Matlab code available on Ilias)\n",
    "\n",
    "For this part, you are allowed to use an existing implementation as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic\n",
    "*Grading: 5 pts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE\n",
    "\n",
    "*Grading: 1 pts*\n",
    "\n",
    "$H_{ML}= -\\sum_{x}\\hat{p}(x)log(\\hat{p}(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_mle(phat):\n",
    "    \"\"\"Maximum likelihood or plug-in estimator of discrete entropy\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "\n",
    "    phat: np.array, shape=(n_bins, )\n",
    "        Estimate of the distribution / histogram\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    H: float\n",
    "        Entropy estimate\n",
    "    \"\"\"\n",
    "    # insert your code here (1 pt)\n",
    "    phat = phat[phat > 0] # avoid log(0)\n",
    "    H = -np.sum(phat * np.log(phat))\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miller-Maddow corrected\n",
    "\n",
    "*Grading: 1 pts*\n",
    "\n",
    "$H_{MM}=H_{ML}+\\frac{\\hat{d}-1}{2n}$ \n",
    "\n",
    "$ \\hat{d} = \\#[\\hat{p}(x)>0]$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_mm(phat, n):\n",
    "    \"\"\"Miller-Maddow corrected estimator of discrete entropy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    phat: np.array, shape=(n_bins, )\n",
    "        Estimate of the distribution / histogram\n",
    "\n",
    "    n:  int\n",
    "        Number of samples\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    H: float\n",
    "        Entropy estimate\n",
    "    \"\"\"\n",
    "\n",
    "    # insert your code here (1 pt)\n",
    "    dhat = np.sum(phat > 0)\n",
    "    H = entropy_mle(phat) + (dhat - 1) / (2 * n)\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jack-knife corrected\n",
    "\n",
    "*Grading: 2 pts*\n",
    "\n",
    "$\\hat{H}_{JK} = N\\hat{H}_{ML} - (N-1)\\hat{H}_{ML}^{(.)}$ \n",
    "\n",
    "$\\hat{H}_{ML}^{(.)} = <H_{ML}^{\\lnot i}>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_jk(x, edges):\n",
    "    \"\"\"Jack-knife corrected estimator of discrete entropy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: np.array, shape=(n_samples, )\n",
    "        Samples\n",
    "\n",
    "    edges: np.array, shape=(n_bins, )\n",
    "        Histogram bin edges\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    H: float\n",
    "        Entropy estimate\n",
    "    \"\"\"\n",
    "\n",
    "    # insert your code here (2 pt)\n",
    "    n = x.size\n",
    "    phat = np.histogram(x, bins=edges)[0] / n\n",
    "\n",
    "    # calculate the maximum likelihood entropy\n",
    "    H_ML = entropy_mle(phat)\n",
    "\n",
    "    H_JK = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        # remove one sample\n",
    "        phat_i = np.histogram(np.delete(x, i), bins=edges)[0] / (n - 1)\n",
    "        # calculate entropy of the reduced sample\n",
    "        H_JK[i] = entropy_mle(phat_i)\n",
    "    \n",
    "    # calculate the jack-knife corrected entropy\n",
    "    H = n * H_ML - (n - 1) * np.mean(H_JK)\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage-adjusted\n",
    "\n",
    "*Grading: 1 pts*\n",
    "\n",
    "$C = 1 - \\frac{\\# f_{i}=1}{N}$\n",
    "\n",
    "$\\hat{P}_{C}= \\hat{P}\\cdot C$ \n",
    "\n",
    "$H_{CA}= -\\sum_{x}\\frac{\\hat{P_{C}}(x)log(\\hat{P_{C}}(x))}{1-(1-\\hat{P_{C}}(x))^N}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_cae(phat, n):\n",
    "    \"\"\"coverage-adjusted estimator of discrete entropy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    phat: np.array, shape=(n_bins, )\n",
    "        Estimate of the distribution / histogram\n",
    "\n",
    "    n: int\n",
    "        Number of samples.\n",
    "\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "\n",
    "    H: float\n",
    "        Entropy estimate\n",
    "    \"\"\"\n",
    "\n",
    "    # insert your code here (1 pt)\n",
    "    \n",
    "    #  estimate the coverage probability C\n",
    "    f1 = np.sum(phat == 1/n)  # Number of singletons (frequency of 1)\n",
    "    C = 1 - f1/n\n",
    "\n",
    "    phat_C = phat * C\n",
    "\n",
    "    H = -np.sum((phat_C * np.log(phat_C)) / (1 - (1 - phat_C)**n))\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced\n",
    "*Grading: 1 + 3 pts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JVHW\n",
    "*Grading: 1 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_jvhw(x):\n",
    "    \"\"\"JVHW estimator of discrete entropy.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    x: np.array, shape=(n_samples, )\n",
    "        Samples\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    H: float\n",
    "        Entropy estimate\n",
    "    \"\"\"\n",
    "\n",
    "    # insert your code here (1 pt)\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus points: Unseen or Best Upper Bounds estimator\n",
    "*Grading: 3 bonus pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "\n",
    "# ------------------------------------------\n",
    "# Port Unseen or Best Upper Bounds estimator\n",
    "# from MatLab to Python. (3 bonus pts)\n",
    "# ------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Distributions\n",
    "*Grading: 4 pts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniform distribution\n",
    "*Grading: 2 pts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "D = 10\n",
    "N = 2**D\n",
    "\n",
    "p = 1 / N * np.ones(N)  # true distribution\n",
    "\n",
    "H = -np.sum(p * np.log2(p))  # true entropy\n",
    "\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the uniform distribution using sample sizes of 100 and 10000. Plot the true distribution and the sampled distributions. What do you notice? (2 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# ------------------------------------\n",
    "# Sample from the uniform distribution\n",
    "# using sample size of 100 (0.5 pts)\n",
    "# ------------------------------------\n",
    "\n",
    "# ------------------------------------\n",
    "# Sample from the uniform distribution\n",
    "# using sample size of 10000 (0.5 pts)\n",
    "# ------------------------------------\n",
    "\n",
    "# ------------------------------------\n",
    "# Plot the true distribution and\n",
    "# the sampled distributions. (0.5 pts)\n",
    "# ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the framework below to generate samples of different size (logarithmically spaced between 10 and 100000) and evaluate the different entropy estimators for multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = np.round(np.logspace(1, 5, num=10))\n",
    "n_runs = 30\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "edges = np.arange(-0.5, N, 1)\n",
    "\n",
    "h_mle = np.zeros((len(sample_sizes), n_runs))\n",
    "h_mm = np.zeros((len(sample_sizes), n_runs))\n",
    "h_jk = np.zeros((len(sample_sizes), n_runs))\n",
    "h_cae = np.zeros((len(sample_sizes), n_runs))\n",
    "h_jvhw = np.zeros((len(sample_sizes), n_runs))\n",
    "\n",
    "# add h_unseen or h_bub here if you implemented them.\n",
    "\n",
    "for i, S in enumerate(sample_sizes):\n",
    "    for j in np.arange(n_runs):\n",
    "\n",
    "        # ------------------------------------\n",
    "        # Sample from the uniform distribution\n",
    "        # with different sample size (0.5 pts)\n",
    "        # ------------------------------------\n",
    "\n",
    "        # insert your code here\n",
    "\n",
    "        h_mle[i, j] = entropy_mle(phat)\n",
    "        h_mm[i, j] = entropy_mm(phat, S)\n",
    "        h_cae[i, j] = entropy_cae(phat, S)\n",
    "        h_jk[i, j] = entropy_jk(x, edges)\n",
    "        h_jvhw[i, j] = entropy_jvhw(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting average estimate of the entropy for each of the estimators. Which is best? If you implemented everything correctly, this plot should roughly look like in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.semilogx(sample_sizes, np.mean(h_mle, axis=1), label=\"mle\")\n",
    "plt.semilogx(sample_sizes, np.mean(h_mm, axis=1), label=\"mm\")\n",
    "plt.semilogx(sample_sizes, np.mean(h_cae, axis=1), label=\"cae\")\n",
    "plt.semilogx(sample_sizes, np.mean(h_jk, axis=1), label=\"jk\")\n",
    "plt.semilogx(sample_sizes, np.mean(h_jvhw, axis=1), label=\"jvhw\")\n",
    "\n",
    "# plot h_unseen or h_bub here if you implemented them.\n",
    "\n",
    "plt.axhline(H, color=\"black\", linestyle=\":\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf distribution\n",
    "*Grading: 2 pts*\n",
    "\n",
    "[Zipf's law ](https://en.wikipedia.org/wiki/Zipf%27s_law) refers to a family of power law like distributions for which $p_k \\sim 1/k^d$. We will simply use $d=1$ here.   \n",
    "\n",
    "Adapt the framework above to sample from a Zipf distribution and evaluate the estimators for this case. Are there differences to the uniform case? (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.510649703297878\n"
     ]
    }
   ],
   "source": [
    "D = 10\n",
    "N = 2**D\n",
    "\n",
    "p = 1 / (np.arange(0, N) + 1)  # true distribution\n",
    "p = p / np.sum(p)\n",
    "\n",
    "H = -np.sum(p * np.log2(p))  # true entropy\n",
    "\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the Zipf distribution using sample sizes of 100 and 10000. In this case, the function `choice` of numpys random number generator is very helpful for sampling. Plot the true distribution and the sampled distributions. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here\n",
    "rng = np.random.default_rng(2)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Sample from the Zipf distribution\n",
    "# using sample size of 100 (0.5 pts)\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Sample from the Zipf distribution\n",
    "# using sample size of 10000 (0.5 pts)\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# Plot the true distribution and the sampled\n",
    "# distributions. (0.5 pts)\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the framework below to generate samples of different size (logarithmically spaced between 10 and 100000) and evaluate the different entropy estimators for multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = np.round(np.logspace(1, 5, num=10))\n",
    "n_runs = 30\n",
    "rng = np.random.default_rng(3)\n",
    "\n",
    "edges = np.arange(-0.5, N, 1)\n",
    "\n",
    "h_mle = np.zeros((len(sample_sizes), n_runs))\n",
    "h_mm = np.zeros((len(sample_sizes), n_runs))\n",
    "h_jk = np.zeros((len(sample_sizes), n_runs))\n",
    "h_cae = np.zeros((len(sample_sizes), n_runs))\n",
    "h_jvhw = np.zeros((len(sample_sizes), n_runs))\n",
    "\n",
    "# add h_unseen or h_bub here if you implemented them.\n",
    "\n",
    "for i, S in enumerate(sample_sizes):\n",
    "    for j in np.arange(n_runs):\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # Sample from the Zipf distribution\n",
    "        # with different sample size (0.5 pts)\n",
    "        # ---------------------------------------\n",
    "\n",
    "        # insert your code here\n",
    "\n",
    "        h_mle[i,j]  = entropy_mle(phat)\n",
    "        h_mm[i,j]   = entropy_mm(phat, S)\n",
    "        h_cae[i,j]  = entropy_cae(phat, S)\n",
    "        h_jk[i,j]   = entropy_jk(x, edges)\n",
    "        h_jvhw[i,j] = entropy_jvhw(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot resulting average estimate of the entropy for each of the estimators. Which is best? If you implemented everything correctly, this plot should roughly look like in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.semilogx(sample_sizes, np.mean(h_mle, axis=1), label=\"mle\")\n",
    "plt.semilogx(sample_sizes, np.mean(h_mm, axis=1), label=\"mm\")\n",
    "plt.semilogx(sample_sizes, np.mean(h_cae, axis=1), label=\"cae\")\n",
    "plt.semilogx(sample_sizes, np.mean(h_jk, axis=1), label=\"jk\")\n",
    "plt.semilogx(sample_sizes, np.mean(h_jvhw, axis=1), label=\"jvhw\")\n",
    "\n",
    "# plot h_unseen or h_bub here if you implemented them.\n",
    "\n",
    "plt.axhline(H, color=\"black\", linestyle=\":\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
